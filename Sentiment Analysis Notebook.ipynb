{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8opKK1P44_A"
      },
      "source": [
        "**Install Library yang diperlukan**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8bnAY7h4-E3",
        "outputId": "1c16363f-b298-4ed6-de8f-26c70f763e13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting tweet-preprocessor\n",
            "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from bs4) (4.13.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->bs4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->bs4) (4.14.1)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.2\n",
            "Get:1 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,628 B]\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,937 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,783 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,191 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,271 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,566 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,235 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,253 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,575 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,422 kB]\n",
            "Fetched 34.6 MB in 4s (8,235 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ca-certificates is already the newest version (20240203~22.04.1).\n",
            "curl is already the newest version (7.81.0-1ubuntu1.20).\n",
            "gnupg is already the newest version (2.2.27-3ubuntu2.4).\n",
            "gnupg set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 74 not upgraded.\n",
            "deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_20.x nodistro main\n",
            "Get:1 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Get:4 https://deb.nodesource.com/node_20.x nodistro InRelease [12.1 kB]\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:7 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Get:9 https://deb.nodesource.com/node_20.x nodistro/main amd64 Packages [12.4 kB]\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 28.5 kB in 1s (21.5 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  nodejs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 74 not upgraded.\n",
            "Need to get 32.0 MB of archives.\n",
            "After this operation, 199 MB of additional disk space will be used.\n",
            "Get:1 https://deb.nodesource.com/node_20.x nodistro/main amd64 nodejs amd64 20.19.4-1nodesource1 [32.0 MB]\n",
            "Fetched 32.0 MB in 1s (58.6 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package nodejs.\n",
            "(Reading database ... 126371 files and directories currently installed.)\n",
            "Preparing to unpack .../nodejs_20.19.4-1nodesource1_amd64.deb ...\n",
            "Unpacking nodejs (20.19.4-1nodesource1) ...\n",
            "Setting up nodejs (20.19.4-1nodesource1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[31merror\u001b[39m \u001b[94mcode\u001b[39m ENOENT\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[31merror\u001b[39m \u001b[94msyscall\u001b[39m open\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[31merror\u001b[39m \u001b[94mpath\u001b[39m /content/package.json\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[31merror\u001b[39m \u001b[94merrno\u001b[39m \u001b[33m-2\u001b[39m\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[31merror\u001b[39m \u001b[94menoent\u001b[39m Could not read package.json: Error: ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[31merror\u001b[39m \u001b[94menoent\u001b[39m This is related to npm not being able to find a file.\n",
            "\u001b[1mnpm\u001b[22m \u001b[31merror\u001b[39m \u001b[94menoent\u001b[39m\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[31merror\u001b[39m A complete log of this run can be found in: /root/.npm/_logs/2025-08-21T09_28_02_387Z-debug-0.log\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0KUnknown command: \"playwright\"\n",
            "\n",
            "To see a list of supported npm commands, run:\n",
            "  npm help\n",
            "\u001b[1G\u001b[0K\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "added 3 packages in 3s\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0KDownloading Chromium 140.0.7339.16 (playwright build v1187)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1187/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G173.7 MiB [] 0% 0.0s\u001b[0K\u001b[1G173.7 MiB [] 0% 54.8s\u001b[0K\u001b[1G173.7 MiB [] 0% 30.9s\u001b[0K\u001b[1G173.7 MiB [] 0% 16.0s\u001b[0K\u001b[1G173.7 MiB [] 0% 9.8s\u001b[0K\u001b[1G173.7 MiB [] 1% 7.4s\u001b[0K\u001b[1G173.7 MiB [] 1% 6.1s\u001b[0K\u001b[1G173.7 MiB [] 2% 5.5s\u001b[0K\u001b[1G173.7 MiB [] 2% 5.7s\u001b[0K\u001b[1G173.7 MiB [] 2% 5.8s\u001b[0K\u001b[1G173.7 MiB [] 2% 5.6s\u001b[0K\u001b[1G173.7 MiB [] 3% 5.2s\u001b[0K\u001b[1G173.7 MiB [] 3% 5.0s\u001b[0K\u001b[1G173.7 MiB [] 4% 4.7s\u001b[0K\u001b[1G173.7 MiB [] 4% 4.6s\u001b[0K\u001b[1G173.7 MiB [] 5% 4.3s\u001b[0K\u001b[1G173.7 MiB [] 6% 4.1s\u001b[0K\u001b[1G173.7 MiB [] 6% 4.0s\u001b[0K\u001b[1G173.7 MiB [] 7% 3.8s\u001b[0K\u001b[1G173.7 MiB [] 7% 3.6s\u001b[0K\u001b[1G173.7 MiB [] 8% 3.6s\u001b[0K\u001b[1G173.7 MiB [] 10% 3.1s\u001b[0K\u001b[1G173.7 MiB [] 10% 3.0s\u001b[0K\u001b[1G173.7 MiB [] 11% 2.9s\u001b[0K\u001b[1G173.7 MiB [] 12% 2.9s\u001b[0K\u001b[1G173.7 MiB [] 12% 3.0s\u001b[0K\u001b[1G173.7 MiB [] 13% 3.0s\u001b[0K\u001b[1G173.7 MiB [] 13% 2.9s\u001b[0K\u001b[1G173.7 MiB [] 14% 2.8s\u001b[0K\u001b[1G173.7 MiB [] 15% 2.8s\u001b[0K\u001b[1G173.7 MiB [] 15% 2.7s\u001b[0K\u001b[1G173.7 MiB [] 16% 2.7s\u001b[0K\u001b[1G173.7 MiB [] 17% 2.6s\u001b[0K\u001b[1G173.7 MiB [] 18% 2.6s\u001b[0K\u001b[1G173.7 MiB [] 18% 2.5s\u001b[0K\u001b[1G173.7 MiB [] 19% 2.4s\u001b[0K\u001b[1G173.7 MiB [] 20% 2.4s\u001b[0K\u001b[1G173.7 MiB [] 21% 2.3s\u001b[0K\u001b[1G173.7 MiB [] 22% 2.3s\u001b[0K\u001b[1G173.7 MiB [] 23% 2.2s\u001b[0K\u001b[1G173.7 MiB [] 24% 2.2s\u001b[0K\u001b[1G173.7 MiB [] 24% 2.1s\u001b[0K\u001b[1G173.7 MiB [] 26% 2.0s\u001b[0K\u001b[1G173.7 MiB [] 27% 2.0s\u001b[0K\u001b[1G173.7 MiB [] 28% 1.9s\u001b[0K\u001b[1G173.7 MiB [] 29% 1.9s\u001b[0K\u001b[1G173.7 MiB [] 30% 1.9s\u001b[0K\u001b[1G173.7 MiB [] 30% 1.8s\u001b[0K\u001b[1G173.7 MiB [] 31% 1.8s\u001b[0K\u001b[1G173.7 MiB [] 32% 1.8s\u001b[0K\u001b[1G173.7 MiB [] 33% 1.7s\u001b[0K\u001b[1G173.7 MiB [] 34% 1.7s\u001b[0K\u001b[1G173.7 MiB [] 35% 1.6s\u001b[0K\u001b[1G173.7 MiB [] 36% 1.6s\u001b[0K\u001b[1G173.7 MiB [] 37% 1.6s\u001b[0K\u001b[1G173.7 MiB [] 38% 1.6s\u001b[0K\u001b[1G173.7 MiB [] 39% 1.5s\u001b[0K\u001b[1G173.7 MiB [] 41% 1.4s\u001b[0K\u001b[1G173.7 MiB [] 42% 1.4s\u001b[0K\u001b[1G173.7 MiB [] 43% 1.4s\u001b[0K\u001b[1G173.7 MiB [] 44% 1.3s\u001b[0K\u001b[1G173.7 MiB [] 45% 1.3s\u001b[0K\u001b[1G173.7 MiB [] 46% 1.3s\u001b[0K\u001b[1G173.7 MiB [] 47% 1.2s\u001b[0K\u001b[1G173.7 MiB [] 48% 1.2s\u001b[0K\u001b[1G173.7 MiB [] 49% 1.2s\u001b[0K\u001b[1G173.7 MiB [] 50% 1.1s\u001b[0K\u001b[1G173.7 MiB [] 51% 1.1s\u001b[0K\u001b[1G173.7 MiB [] 52% 1.1s\u001b[0K\u001b[1G173.7 MiB [] 53% 1.1s\u001b[0K\u001b[1G173.7 MiB [] 54% 1.0s\u001b[0K\u001b[1G173.7 MiB [] 55% 1.0s\u001b[0K\u001b[1G173.7 MiB [] 56% 1.0s\u001b[0K\u001b[1G173.7 MiB [] 57% 1.0s\u001b[0K\u001b[1G173.7 MiB [] 58% 0.9s\u001b[0K\u001b[1G173.7 MiB [] 59% 0.9s\u001b[0K\u001b[1G173.7 MiB [] 60% 0.9s\u001b[0K\u001b[1G173.7 MiB [] 61% 0.9s\u001b[0K\u001b[1G173.7 MiB [] 62% 0.8s\u001b[0K\u001b[1G173.7 MiB [] 63% 0.8s\u001b[0K\u001b[1G173.7 MiB [] 64% 0.8s\u001b[0K\u001b[1G173.7 MiB [] 65% 0.8s\u001b[0K\u001b[1G173.7 MiB [] 66% 0.7s\u001b[0K\u001b[1G173.7 MiB [] 68% 0.7s\u001b[0K\u001b[1G173.7 MiB [] 69% 0.7s\u001b[0K\u001b[1G173.7 MiB [] 70% 0.6s\u001b[0K\u001b[1G173.7 MiB [] 71% 0.6s\u001b[0K\u001b[1G173.7 MiB [] 72% 0.6s\u001b[0K\u001b[1G173.7 MiB [] 73% 0.6s\u001b[0K\u001b[1G173.7 MiB [] 74% 0.5s\u001b[0K\u001b[1G173.7 MiB [] 75% 0.5s\u001b[0K\u001b[1G173.7 MiB [] 76% 0.5s\u001b[0K\u001b[1G173.7 MiB [] 77% 0.5s\u001b[0K\u001b[1G173.7 MiB [] 78% 0.5s\u001b[0K\u001b[1G173.7 MiB [] 79% 0.4s\u001b[0K\u001b[1G173.7 MiB [] 80% 0.4s\u001b[0K\u001b[1G173.7 MiB [] 81% 0.4s\u001b[0K\u001b[1G173.7 MiB [] 82% 0.4s\u001b[0K\u001b[1G173.7 MiB [] 83% 0.4s\u001b[0K\u001b[1G173.7 MiB [] 83% 0.3s\u001b[0K\u001b[1G173.7 MiB [] 84% 0.3s\u001b[0K\u001b[1G173.7 MiB [] 85% 0.3s\u001b[0K\u001b[1G173.7 MiB [] 86% 0.3s\u001b[0K\u001b[1G173.7 MiB [] 87% 0.3s\u001b[0K\u001b[1G173.7 MiB [] 88% 0.2s\u001b[0K\u001b[1G173.7 MiB [] 89% 0.2s\u001b[0K\u001b[1G173.7 MiB [] 90% 0.2s\u001b[0K\u001b[1G173.7 MiB [] 91% 0.2s\u001b[0K\u001b[1G173.7 MiB [] 92% 0.2s\u001b[0K\u001b[1G173.7 MiB [] 93% 0.1s\u001b[0K\u001b[1G173.7 MiB [] 94% 0.1s\u001b[0K\u001b[1G173.7 MiB [] 95% 0.1s\u001b[0K\u001b[1G173.7 MiB [] 96% 0.1s\u001b[0K\u001b[1G173.7 MiB [] 97% 0.1s\u001b[0K\u001b[1G173.7 MiB [] 97% 0.0s\u001b[0K\u001b[1G173.7 MiB [] 99% 0.0s\u001b[0K\u001b[1G173.7 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 140.0.7339.16 (playwright build v1187) downloaded to /root/.cache/ms-playwright/chromium-1187\n",
            "Downloading Chromium Headless Shell 140.0.7339.16 (playwright build v1187)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1187/chromium-headless-shell-linux.zip\u001b[22m\n",
            "\u001b[1G104.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G104.3 MiB [] 0% 27.3s\u001b[0K\u001b[1G104.3 MiB [] 0% 17.5s\u001b[0K\u001b[1G104.3 MiB [] 0% 10.4s\u001b[0K\u001b[1G104.3 MiB [] 0% 7.4s\u001b[0K\u001b[1G104.3 MiB [] 1% 5.8s\u001b[0K\u001b[1G104.3 MiB [] 1% 4.9s\u001b[0K\u001b[1G104.3 MiB [] 2% 4.5s\u001b[0K\u001b[1G104.3 MiB [] 3% 4.1s\u001b[0K\u001b[1G104.3 MiB [] 3% 3.9s\u001b[0K\u001b[1G104.3 MiB [] 4% 3.8s\u001b[0K\u001b[1G104.3 MiB [] 5% 3.6s\u001b[0K\u001b[1G104.3 MiB [] 5% 3.7s\u001b[0K\u001b[1G104.3 MiB [] 6% 3.7s\u001b[0K\u001b[1G104.3 MiB [] 7% 3.6s\u001b[0K\u001b[1G104.3 MiB [] 8% 3.5s\u001b[0K\u001b[1G104.3 MiB [] 8% 3.3s\u001b[0K\u001b[1G104.3 MiB [] 9% 3.4s\u001b[0K\u001b[1G104.3 MiB [] 9% 3.3s\u001b[0K\u001b[1G104.3 MiB [] 10% 3.3s\u001b[0K\u001b[1G104.3 MiB [] 11% 3.2s\u001b[0K\u001b[1G104.3 MiB [] 12% 3.1s\u001b[0K\u001b[1G104.3 MiB [] 13% 3.0s\u001b[0K\u001b[1G104.3 MiB [] 14% 3.0s\u001b[0K\u001b[1G104.3 MiB [] 15% 2.9s\u001b[0K\u001b[1G104.3 MiB [] 16% 2.8s\u001b[0K\u001b[1G104.3 MiB [] 17% 2.8s\u001b[0K\u001b[1G104.3 MiB [] 18% 2.8s\u001b[0K\u001b[1G104.3 MiB [] 18% 2.7s\u001b[0K\u001b[1G104.3 MiB [] 19% 2.7s\u001b[0K\u001b[1G104.3 MiB [] 20% 2.7s\u001b[0K\u001b[1G104.3 MiB [] 21% 2.6s\u001b[0K\u001b[1G104.3 MiB [] 21% 2.7s\u001b[0K\u001b[1G104.3 MiB [] 22% 2.6s\u001b[0K\u001b[1G104.3 MiB [] 23% 2.6s\u001b[0K\u001b[1G104.3 MiB [] 24% 2.5s\u001b[0K\u001b[1G104.3 MiB [] 25% 2.5s\u001b[0K\u001b[1G104.3 MiB [] 26% 2.4s\u001b[0K\u001b[1G104.3 MiB [] 27% 2.4s\u001b[0K\u001b[1G104.3 MiB [] 28% 2.4s\u001b[0K\u001b[1G104.3 MiB [] 29% 2.3s\u001b[0K\u001b[1G104.3 MiB [] 30% 2.2s\u001b[0K\u001b[1G104.3 MiB [] 31% 2.2s\u001b[0K\u001b[1G104.3 MiB [] 32% 2.1s\u001b[0K\u001b[1G104.3 MiB [] 33% 2.1s\u001b[0K\u001b[1G104.3 MiB [] 34% 2.0s\u001b[0K\u001b[1G104.3 MiB [] 36% 1.9s\u001b[0K\u001b[1G104.3 MiB [] 37% 1.8s\u001b[0K\u001b[1G104.3 MiB [] 39% 1.7s\u001b[0K\u001b[1G104.3 MiB [] 41% 1.6s\u001b[0K\u001b[1G104.3 MiB [] 42% 1.5s\u001b[0K\u001b[1G104.3 MiB [] 43% 1.5s\u001b[0K\u001b[1G104.3 MiB [] 44% 1.5s\u001b[0K\u001b[1G104.3 MiB [] 45% 1.5s\u001b[0K\u001b[1G104.3 MiB [] 46% 1.4s\u001b[0K\u001b[1G104.3 MiB [] 48% 1.3s\u001b[0K\u001b[1G104.3 MiB [] 50% 1.3s\u001b[0K\u001b[1G104.3 MiB [] 50% 1.2s\u001b[0K\u001b[1G104.3 MiB [] 51% 1.2s\u001b[0K\u001b[1G104.3 MiB [] 52% 1.2s\u001b[0K\u001b[1G104.3 MiB [] 54% 1.1s\u001b[0K\u001b[1G104.3 MiB [] 56% 1.0s\u001b[0K\u001b[1G104.3 MiB [] 57% 1.0s\u001b[0K\u001b[1G104.3 MiB [] 59% 0.9s\u001b[0K\u001b[1G104.3 MiB [] 61% 0.9s\u001b[0K\u001b[1G104.3 MiB [] 62% 0.8s\u001b[0K\u001b[1G104.3 MiB [] 64% 0.8s\u001b[0K\u001b[1G104.3 MiB [] 65% 0.7s\u001b[0K\u001b[1G104.3 MiB [] 66% 0.7s\u001b[0K\u001b[1G104.3 MiB [] 68% 0.7s\u001b[0K\u001b[1G104.3 MiB [] 69% 0.7s\u001b[0K\u001b[1G104.3 MiB [] 70% 0.6s\u001b[0K\u001b[1G104.3 MiB [] 72% 0.6s\u001b[0K\u001b[1G104.3 MiB [] 73% 0.5s\u001b[0K\u001b[1G104.3 MiB [] 75% 0.5s\u001b[0K\u001b[1G104.3 MiB [] 77% 0.5s\u001b[0K\u001b[1G104.3 MiB [] 79% 0.4s\u001b[0K\u001b[1G104.3 MiB [] 80% 0.4s\u001b[0K\u001b[1G104.3 MiB [] 82% 0.3s\u001b[0K\u001b[1G104.3 MiB [] 83% 0.3s\u001b[0K\u001b[1G104.3 MiB [] 85% 0.3s\u001b[0K\u001b[1G104.3 MiB [] 87% 0.2s\u001b[0K\u001b[1G104.3 MiB [] 88% 0.2s\u001b[0K\u001b[1G104.3 MiB [] 90% 0.2s\u001b[0K\u001b[1G104.3 MiB [] 92% 0.1s\u001b[0K\u001b[1G104.3 MiB [] 93% 0.1s\u001b[0K\u001b[1G104.3 MiB [] 95% 0.1s\u001b[0K\u001b[1G104.3 MiB [] 97% 0.1s\u001b[0K\u001b[1G104.3 MiB [] 98% 0.0s\u001b[0K\u001b[1G104.3 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium Headless Shell 140.0.7339.16 (playwright build v1187) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1187\n",
            "Downloading Firefox 141.0 (playwright build v1490)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/firefox/1490/firefox-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G96 MiB [] 0% 0.0s\u001b[0K\u001b[1G96 MiB [] 0% 26.5s\u001b[0K\u001b[1G96 MiB [] 0% 16.6s\u001b[0K\u001b[1G96 MiB [] 0% 11.1s\u001b[0K\u001b[1G96 MiB [] 0% 8.7s\u001b[0K\u001b[1G96 MiB [] 1% 8.0s\u001b[0K\u001b[1G96 MiB [] 1% 6.5s\u001b[0K\u001b[1G96 MiB [] 2% 5.3s\u001b[0K\u001b[1G96 MiB [] 2% 4.8s\u001b[0K\u001b[1G96 MiB [] 3% 4.6s\u001b[0K\u001b[1G96 MiB [] 3% 4.3s\u001b[0K\u001b[1G96 MiB [] 4% 4.3s\u001b[0K\u001b[1G96 MiB [] 4% 4.2s\u001b[0K\u001b[1G96 MiB [] 5% 4.1s\u001b[0K\u001b[1G96 MiB [] 6% 3.9s\u001b[0K\u001b[1G96 MiB [] 7% 3.7s\u001b[0K\u001b[1G96 MiB [] 8% 3.5s\u001b[0K\u001b[1G96 MiB [] 9% 3.3s\u001b[0K\u001b[1G96 MiB [] 10% 3.1s\u001b[0K\u001b[1G96 MiB [] 11% 2.9s\u001b[0K\u001b[1G96 MiB [] 12% 2.8s\u001b[0K\u001b[1G96 MiB [] 13% 2.7s\u001b[0K\u001b[1G96 MiB [] 14% 2.6s\u001b[0K\u001b[1G96 MiB [] 15% 2.5s\u001b[0K\u001b[1G96 MiB [] 16% 2.4s\u001b[0K\u001b[1G96 MiB [] 17% 2.3s\u001b[0K\u001b[1G96 MiB [] 18% 2.3s\u001b[0K\u001b[1G96 MiB [] 19% 2.2s\u001b[0K\u001b[1G96 MiB [] 20% 2.1s\u001b[0K\u001b[1G96 MiB [] 21% 2.1s\u001b[0K\u001b[1G96 MiB [] 22% 2.0s\u001b[0K\u001b[1G96 MiB [] 23% 2.0s\u001b[0K\u001b[1G96 MiB [] 24% 1.9s\u001b[0K\u001b[1G96 MiB [] 25% 1.9s\u001b[0K\u001b[1G96 MiB [] 26% 1.9s\u001b[0K\u001b[1G96 MiB [] 27% 1.8s\u001b[0K\u001b[1G96 MiB [] 28% 1.7s\u001b[0K\u001b[1G96 MiB [] 29% 1.7s\u001b[0K\u001b[1G96 MiB [] 30% 1.7s\u001b[0K\u001b[1G96 MiB [] 31% 1.6s\u001b[0K\u001b[1G96 MiB [] 32% 1.6s\u001b[0K\u001b[1G96 MiB [] 33% 1.6s\u001b[0K\u001b[1G96 MiB [] 33% 1.5s\u001b[0K\u001b[1G96 MiB [] 34% 1.5s\u001b[0K\u001b[1G96 MiB [] 35% 1.5s\u001b[0K\u001b[1G96 MiB [] 36% 1.4s\u001b[0K\u001b[1G96 MiB [] 37% 1.4s\u001b[0K\u001b[1G96 MiB [] 38% 1.4s\u001b[0K\u001b[1G96 MiB [] 39% 1.4s\u001b[0K\u001b[1G96 MiB [] 40% 1.3s\u001b[0K\u001b[1G96 MiB [] 41% 1.3s\u001b[0K\u001b[1G96 MiB [] 42% 1.3s\u001b[0K\u001b[1G96 MiB [] 43% 1.2s\u001b[0K\u001b[1G96 MiB [] 45% 1.2s\u001b[0K\u001b[1G96 MiB [] 47% 1.1s\u001b[0K\u001b[1G96 MiB [] 48% 1.1s\u001b[0K\u001b[1G96 MiB [] 49% 1.1s\u001b[0K\u001b[1G96 MiB [] 50% 1.0s\u001b[0K\u001b[1G96 MiB [] 51% 1.0s\u001b[0K\u001b[1G96 MiB [] 53% 1.0s\u001b[0K\u001b[1G96 MiB [] 54% 0.9s\u001b[0K\u001b[1G96 MiB [] 55% 0.9s\u001b[0K\u001b[1G96 MiB [] 56% 0.9s\u001b[0K\u001b[1G96 MiB [] 58% 0.8s\u001b[0K\u001b[1G96 MiB [] 59% 0.8s\u001b[0K\u001b[1G96 MiB [] 60% 0.8s\u001b[0K\u001b[1G96 MiB [] 62% 0.7s\u001b[0K\u001b[1G96 MiB [] 63% 0.7s\u001b[0K\u001b[1G96 MiB [] 65% 0.7s\u001b[0K\u001b[1G96 MiB [] 66% 0.6s\u001b[0K\u001b[1G96 MiB [] 68% 0.6s\u001b[0K\u001b[1G96 MiB [] 70% 0.6s\u001b[0K\u001b[1G96 MiB [] 71% 0.5s\u001b[0K\u001b[1G96 MiB [] 73% 0.5s\u001b[0K\u001b[1G96 MiB [] 74% 0.5s\u001b[0K\u001b[1G96 MiB [] 77% 0.4s\u001b[0K\u001b[1G96 MiB [] 78% 0.4s\u001b[0K\u001b[1G96 MiB [] 80% 0.3s\u001b[0K\u001b[1G96 MiB [] 82% 0.3s\u001b[0K\u001b[1G96 MiB [] 83% 0.3s\u001b[0K\u001b[1G96 MiB [] 85% 0.3s\u001b[0K\u001b[1G96 MiB [] 87% 0.2s\u001b[0K\u001b[1G96 MiB [] 88% 0.2s\u001b[0K\u001b[1G96 MiB [] 89% 0.2s\u001b[0K\u001b[1G96 MiB [] 90% 0.2s\u001b[0K\u001b[1G96 MiB [] 91% 0.2s\u001b[0K\u001b[1G96 MiB [] 92% 0.1s\u001b[0K\u001b[1G96 MiB [] 93% 0.1s\u001b[0K\u001b[1G96 MiB [] 94% 0.1s\u001b[0K\u001b[1G96 MiB [] 95% 0.1s\u001b[0K\u001b[1G96 MiB [] 96% 0.1s\u001b[0K\u001b[1G96 MiB [] 97% 0.0s\u001b[0K\u001b[1G96 MiB [] 98% 0.0s\u001b[0K\u001b[1G96 MiB [] 100% 0.0s\u001b[0K\n",
            "Firefox 141.0 (playwright build v1490) downloaded to /root/.cache/ms-playwright/firefox-1490\n",
            "Downloading Webkit 26.0 (playwright build v2203)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/webkit/2203/webkit-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G94.6 MiB [] 0% 0.0s\u001b[0K\u001b[1G94.6 MiB [] 0% 26.1s\u001b[0K\u001b[1G94.6 MiB [] 0% 16.3s\u001b[0K\u001b[1G94.6 MiB [] 0% 9.8s\u001b[0K\u001b[1G94.6 MiB [] 0% 6.8s\u001b[0K\u001b[1G94.6 MiB [] 1% 5.0s\u001b[0K\u001b[1G94.6 MiB [] 2% 4.3s\u001b[0K\u001b[1G94.6 MiB [] 2% 4.0s\u001b[0K\u001b[1G94.6 MiB [] 3% 3.8s\u001b[0K\u001b[1G94.6 MiB [] 3% 3.6s\u001b[0K\u001b[1G94.6 MiB [] 4% 3.5s\u001b[0K\u001b[1G94.6 MiB [] 4% 3.6s\u001b[0K\u001b[1G94.6 MiB [] 5% 3.6s\u001b[0K\u001b[1G94.6 MiB [] 6% 3.7s\u001b[0K\u001b[1G94.6 MiB [] 6% 3.8s\u001b[0K\u001b[1G94.6 MiB [] 6% 3.7s\u001b[0K\u001b[1G94.6 MiB [] 7% 3.4s\u001b[0K\u001b[1G94.6 MiB [] 8% 3.2s\u001b[0K\u001b[1G94.6 MiB [] 9% 3.0s\u001b[0K\u001b[1G94.6 MiB [] 10% 2.8s\u001b[0K\u001b[1G94.6 MiB [] 11% 2.7s\u001b[0K\u001b[1G94.6 MiB [] 12% 2.5s\u001b[0K\u001b[1G94.6 MiB [] 13% 2.4s\u001b[0K\u001b[1G94.6 MiB [] 14% 2.3s\u001b[0K\u001b[1G94.6 MiB [] 15% 2.3s\u001b[0K\u001b[1G94.6 MiB [] 16% 2.1s\u001b[0K\u001b[1G94.6 MiB [] 17% 2.1s\u001b[0K\u001b[1G94.6 MiB [] 18% 2.1s\u001b[0K\u001b[1G94.6 MiB [] 19% 2.1s\u001b[0K\u001b[1G94.6 MiB [] 19% 2.0s\u001b[0K\u001b[1G94.6 MiB [] 21% 1.9s\u001b[0K\u001b[1G94.6 MiB [] 21% 2.0s\u001b[0K\u001b[1G94.6 MiB [] 22% 1.9s\u001b[0K\u001b[1G94.6 MiB [] 24% 1.9s\u001b[0K\u001b[1G94.6 MiB [] 25% 1.8s\u001b[0K\u001b[1G94.6 MiB [] 26% 1.8s\u001b[0K\u001b[1G94.6 MiB [] 27% 1.7s\u001b[0K\u001b[1G94.6 MiB [] 28% 1.6s\u001b[0K\u001b[1G94.6 MiB [] 28% 1.7s\u001b[0K\u001b[1G94.6 MiB [] 30% 1.5s\u001b[0K\u001b[1G94.6 MiB [] 32% 1.5s\u001b[0K\u001b[1G94.6 MiB [] 33% 1.4s\u001b[0K\u001b[1G94.6 MiB [] 35% 1.4s\u001b[0K\u001b[1G94.6 MiB [] 36% 1.3s\u001b[0K\u001b[1G94.6 MiB [] 37% 1.3s\u001b[0K\u001b[1G94.6 MiB [] 39% 1.2s\u001b[0K\u001b[1G94.6 MiB [] 40% 1.2s\u001b[0K\u001b[1G94.6 MiB [] 41% 1.1s\u001b[0K\u001b[1G94.6 MiB [] 43% 1.1s\u001b[0K\u001b[1G94.6 MiB [] 44% 1.0s\u001b[0K\u001b[1G94.6 MiB [] 45% 1.0s\u001b[0K\u001b[1G94.6 MiB [] 47% 1.0s\u001b[0K\u001b[1G94.6 MiB [] 49% 0.9s\u001b[0K\u001b[1G94.6 MiB [] 51% 0.9s\u001b[0K\u001b[1G94.6 MiB [] 52% 0.8s\u001b[0K\u001b[1G94.6 MiB [] 54% 0.8s\u001b[0K\u001b[1G94.6 MiB [] 55% 0.8s\u001b[0K\u001b[1G94.6 MiB [] 57% 0.7s\u001b[0K\u001b[1G94.6 MiB [] 58% 0.7s\u001b[0K\u001b[1G94.6 MiB [] 59% 0.7s\u001b[0K\u001b[1G94.6 MiB [] 60% 0.7s\u001b[0K\u001b[1G94.6 MiB [] 61% 0.6s\u001b[0K\u001b[1G94.6 MiB [] 62% 0.6s\u001b[0K\u001b[1G94.6 MiB [] 64% 0.6s\u001b[0K\u001b[1G94.6 MiB [] 65% 0.6s\u001b[0K\u001b[1G94.6 MiB [] 67% 0.5s\u001b[0K\u001b[1G94.6 MiB [] 69% 0.5s\u001b[0K\u001b[1G94.6 MiB [] 70% 0.5s\u001b[0K\u001b[1G94.6 MiB [] 71% 0.4s\u001b[0K\u001b[1G94.6 MiB [] 73% 0.4s\u001b[0K\u001b[1G94.6 MiB [] 75% 0.4s\u001b[0K\u001b[1G94.6 MiB [] 76% 0.4s\u001b[0K\u001b[1G94.6 MiB [] 79% 0.3s\u001b[0K\u001b[1G94.6 MiB [] 81% 0.3s\u001b[0K\u001b[1G94.6 MiB [] 82% 0.3s\u001b[0K\u001b[1G94.6 MiB [] 84% 0.2s\u001b[0K\u001b[1G94.6 MiB [] 85% 0.2s\u001b[0K\u001b[1G94.6 MiB [] 87% 0.2s\u001b[0K\u001b[1G94.6 MiB [] 89% 0.2s\u001b[0K\u001b[1G94.6 MiB [] 90% 0.1s\u001b[0K\u001b[1G94.6 MiB [] 92% 0.1s\u001b[0K\u001b[1G94.6 MiB [] 93% 0.1s\u001b[0K\u001b[1G94.6 MiB [] 95% 0.1s\u001b[0K\u001b[1G94.6 MiB [] 97% 0.0s\u001b[0K\u001b[1G94.6 MiB [] 100% 0.0s\u001b[0K\n",
            "Webkit 26.0 (playwright build v2203) downloaded to /root/.cache/ms-playwright/webkit-2203\n",
            "Downloading FFMPEG playwright build v1011\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/ffmpeg/1011/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 2% 0.7s\u001b[0K\u001b[1G2.3 MiB [] 8% 0.4s\u001b[0K\u001b[1G2.3 MiB [] 20% 0.2s\u001b[0K\u001b[1G2.3 MiB [] 40% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 55% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 85% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1011 downloaded to /root/.cache/ms-playwright/ffmpeg-1011\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux \u001b[90m(/content/\u001b[39mnode_modules/\u001b[4mplaywright-core\u001b[24m/lib/server/registry/dependencies.js:269:9\u001b[90m)\u001b[39m\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements \u001b[90m(/content/\u001b[39mnode_modules/\u001b[4mplaywright-core\u001b[24m/lib/server/registry/index.js:934:14\u001b[90m)\u001b[39m\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded \u001b[90m(/content/\u001b[39mnode_modules/\u001b[4mplaywright-core\u001b[24m/lib/server/registry/index.js:1056:7\u001b[90m)\u001b[39m\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded \u001b[90m(/content/\u001b[39mnode_modules/\u001b[4mplaywright-core\u001b[24m/lib/server/registry/index.js:1045:7\u001b[90m)\u001b[39m\n",
            "    at async i.<anonymous> \u001b[90m(/content/\u001b[39mnode_modules/\u001b[4mplaywright-core\u001b[24m/lib/cli/program.js:217:7\u001b[90m)\u001b[39m\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0Kv20.19.4\n",
            "Collecting google-play-scraper\n",
            "  Downloading google_play_scraper-1.2.7-py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_play_scraper-1.2.7-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: google-play-scraper\n",
            "Successfully installed google-play-scraper-1.2.7\n",
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.12/dist-packages (1.9.4)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from wordcloud) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from wordcloud) (11.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from wordcloud) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.100.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas # package manager python install luibrary pandas\n",
        "!pip install tweet-preprocessor\n",
        "!pip install requests bs4\n",
        "\n",
        "# Install Node.js (because tweet-harvest built using Node.js)\n",
        "!sudo apt-get update #Perintah untuk update repository linux\n",
        "!sudo apt-get install -y ca-certificates curl gnupg\n",
        "!sudo mkdir -p /etc/apt/keyrings\n",
        "!curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg\n",
        "\n",
        "!NODE_MAJOR=20 && echo \"deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main\" | sudo tee /etc/apt/sources.list.d/nodesource.list\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install nodejs -y\n",
        "!sudo npm install\n",
        "!sudo npm playwright install\n",
        "!sudo npm install @playwright/test\n",
        "!sudo npx playwright install\n",
        "\n",
        "\n",
        "!node -v\n",
        "\n",
        "!pip install google-play-scraper\n",
        "!pip install Sastrawi\n",
        "#!pip install googletrans==3.1.0a0\n",
        "!pip install tensorflow\n",
        "!pip install transformers\n",
        "!pip install -q -U keras-tuner\n",
        "!pip install transformers torch nltk\n",
        "!pip install wordcloud\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb-E_nAU6F7i"
      },
      "source": [
        "**Mount Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikL3aR4F6MTb",
        "outputId": "8f0dd6be-09de-4880-b100-0aced9c5013f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caximnca6Pme"
      },
      "source": [
        "**Import Library yang dibutuhkan**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0ZCGQixA6Vea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "6c84660b-fd9f-4c24-f4f1-19dcd12b7785"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gensim'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-535215333.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#from googletrans import Translator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import preprocessor as p\n",
        "import requests\n",
        "import urllib\n",
        "import nltk\n",
        "import os\n",
        "import openai\n",
        "import collections\n",
        "import tensorflow as tf\n",
        "from google_play_scraper import app\n",
        "from google_play_scraper import Sort, reviews_all\n",
        "from bs4 import BeautifulSoup\n",
        "from textblob import TextBlob\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from nltk.tokenize import word_tokenize\n",
        "#from googletrans import Translator\n",
        "from __future__ import division, print_function\n",
        "from gensim import models\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from keras import regularizers, backend as K\n",
        "from tensorflow.keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, MaxPooling1D, AveragePooling1D, GlobalAveragePooling1D,Embedding\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import torch\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "from keras_tuner.tuners import BayesianOptimization\n",
        "import IPython\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhcrahch7qqo"
      },
      "source": [
        "**Crawling Twitter**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TlBez1R7vMq",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "filename = 'data twitter #aplikasisolodestination.csv' #penamaan saving file\n",
        "search_keyword = '#aplikasisolodestination' #kata kunci untuk crawling data di pencarian twitter\n",
        "limit = 500 #jumlah maksimal data yang diambil\n",
        "\n",
        "!npx --yes tweet-harvest@2.6.1 -o \"{filename}\" -s \"{search_keyword}\" -l {limit} --token \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQ17Aki_8zpA",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/drive/MyDrive/Bismillah TA/raw data twitter.csv\" #file path untuk data twitter yang sudah di crawling\n",
        "twitter_1 = pd.read_csv(file_path, encoding='latin1')  #membaca file path dalam bentuk data frame\n",
        "display(twitter_1) #display data frame hasil crawling twitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kd_JzIQZ9TX8"
      },
      "outputs": [],
      "source": [
        "df_twitter_1 = twitter_1[['full_text']] #membuat fungsi agar data bisa digabungkan dengan sumber lain\n",
        "df_twitter_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLu-ZNaTXZ50"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/drive/MyDrive/Bismillah TA/raw_data_twitter_ (1).csv\" #file path untuk data twitter yang sudah di crawling\n",
        "twitter_2 = pd.read_csv(file_path, encoding='latin1')  #membaca file path dalam bentuk data frame\n",
        "display(twitter_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWxldxDJXl5q"
      },
      "outputs": [],
      "source": [
        "df_twitter_2 = twitter_2[['full_text']]\n",
        "df_twitter_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNDUG9zLe511"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/drive/MyDrive/Bismillah TA (1)/Data Twitter/data__twitter_apk_solo_destination.csv\" #file path untuk data twitter yang sudah di crawling\n",
        "twitter_3 = pd.read_csv(file_path, encoding='latin1')  #membaca file path dalam bentuk data frame\n",
        "display(twitter_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8z_xUXO5e8ZD"
      },
      "outputs": [],
      "source": [
        "df_twitter_3 = twitter_3[['full_text']]\n",
        "df_twitter_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYxZ4J0VfDm0"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/drive/MyDrive/Bismillah TA (1)/Data Twitter/data_twitter_aplikasi_destination (1).csv\" #file path untuk data twitter yang sudah di crawling\n",
        "twitter_4 = pd.read_csv(file_path, encoding='latin1')  #membaca file path dalam bentuk data frame\n",
        "display(twitter_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDWst47sfJVP"
      },
      "outputs": [],
      "source": [
        "df_twitter_4 = twitter_4[['full_text']]\n",
        "df_twitter_4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdgKFBHZAwHw"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/drive/MyDrive/Bismillah TA (1)/Data Twitter/data_twitter_aplikasi_solo_destinasi (1).csv\" #file path untuk data twitter yang sudah di crawling\n",
        "twitter_5 = pd.read_csv(file_path, encoding='latin1')  #membaca file path dalam bentuk data frame\n",
        "display(twitter_5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlZOseMKBCVg"
      },
      "outputs": [],
      "source": [
        "df_twitter_5 = twitter_5[['full_text']]\n",
        "df_twitter_5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwcLP0ppDwzN"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/drive/MyDrive/Bismillah TA (1)/Data Twitter/data_twitter_aplikasi_solo_destination.csv\" #file path untuk data twitter yang sudah di crawling\n",
        "twitter_6 = pd.read_csv(file_path, encoding='latin1')  #membaca file path dalam bentuk data frame\n",
        "display(twitter_6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIC4jAh2EDGB"
      },
      "outputs": [],
      "source": [
        "df_twitter_6 = twitter_6[['full_text']]\n",
        "df_twitter_6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aURD92koEGQr"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/drive/MyDrive/Bismillah TA (1)/Data Twitter/data_twitter_app_solo_destination.csv\" #file path untuk data twitter yang sudah di crawling\n",
        "twitter_7 = pd.read_csv(file_path, encoding='latin1')  #membaca file path dalam bentuk data frame\n",
        "display(twitter_7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myTot7MpEPY1"
      },
      "outputs": [],
      "source": [
        "df_twitter_7 = twitter_7[['full_text']]\n",
        "df_twitter_7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UWc4kgVG1f7"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/drive/MyDrive/Bismillah TA (1)/Data Twitter/data_twitter_aplikasi_solo_ (1).csv\" #file path untuk data twitter yang sudah di crawling\n",
        "twitter_8 = pd.read_csv(file_path, encoding='latin1')  #membaca file path dalam bentuk data frame\n",
        "display(twitter_8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpnQpctGKAMU"
      },
      "outputs": [],
      "source": [
        "df_twitter_8 = twitter_8[['full_text']]\n",
        "df_twitter_8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TFPYIjzX1uj"
      },
      "outputs": [],
      "source": [
        "frames = [df_twitter_1, df_twitter_2, df_twitter_3, df_twitter_4, df_twitter_5, df_twitter_6, df_twitter_7, df_twitter_8]\n",
        "df_twitter_all = pd.concat(frames)\n",
        "df_twitter_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wid6KqHC9m-f"
      },
      "source": [
        "**Scraping Data dari Google Play Store**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4ts5FnT9rUt"
      },
      "outputs": [],
      "source": [
        "us_reviews = reviews_all(\n",
        "    'com.gamatechno.solodestinationnew', #id untuk aplikasi Solo Destination di Google Play Store\n",
        "    sleep_milliseconds=0,\n",
        "    lang='id',  #memilih bahasa untuk data yang diambil 'id' adalah kode untuk bahasa Indonesia\n",
        "    country='id', #nemilih negara untuk data yang diambil\n",
        "    sort=Sort.NEWEST, #mengambil data dari komentar terbaru\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OB_-WbGa-h9Y"
      },
      "outputs": [],
      "source": [
        "df_gplay = pd.DataFrame(np.array(us_reviews),columns=['review']) #menampilkan hasil scraping google playstore dalam bentuk dataframe\n",
        "df_gplay = df_gplay.join(pd.DataFrame(df_gplay.pop('review').tolist()))\n",
        "df_gplay.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JOdIDt1CPCq"
      },
      "outputs": [],
      "source": [
        "df_gplay.to_csv('Google_PlayStore.csv')\n",
        "!cp Google_PlayStore.csv \"/content/drive/MyDrive/Bismillah TA\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYjt4tkH_nDY"
      },
      "outputs": [],
      "source": [
        "df_gplay_subset = df_gplay[['content']]\n",
        "df_gplay_joined = df_gplay_subset.rename(columns={'content' : 'full_text'}) #mengganti nama kolom 'content' menjadi 'full_text' agar sama dengan kolom pada twitter sehingga bisa digabungkan\n",
        "df_gplay_joined"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vunxxsPLAHff"
      },
      "source": [
        "**Scraping dari Website ULAS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y97Irv-OBlr_"
      },
      "outputs": [],
      "source": [
        "#case folding\n",
        "def to_lower(review_text) :\n",
        "  return review_text.lower()\n",
        "\n",
        "#cleaning\n",
        "def remove_emoji(review_text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           u\"\\U0001f926-\\U0001f937\"\n",
        "                           u\"\\U00010000-\\U0010ffff\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', review_text)\n",
        "\n",
        "def remove_hashtag(review_text, default_replace=\"\"):\n",
        "  hashtag = re.sub(r'#\\w+', default_replace, review_text)\n",
        "  return hashtag\n",
        "\n",
        "def remove_punctuation(review_text, default_text=\" \"):\n",
        "  list_punct = string.punctuation\n",
        "  delete_punct = str.maketrans(list_punct,' '*len(list_punct))\n",
        "  new_review = ' '.join(review_text.translate(delete_punct).split())\n",
        "  return new_review\n",
        "\n",
        "def remove_superscript(review_text):\n",
        "  number = re.compile(\"[\"u\"\\U00002070\"\n",
        "                      u\"\\U000000B9\"\n",
        "                      u\"\\U000000B2-\\U000000B3\"\n",
        "                      u\"\\U00002074-\\U00002079\"\n",
        "                      u\"\\U0000207A-\\U0000207E\"\n",
        "                      u\"U0000200D\"\n",
        "                      \"]+\", flags=re.UNICODE)\n",
        "  return number.sub(r'', review_text)\n",
        "\n",
        "def word_repetition(review_text):\n",
        "  review = re.sub(r'(.)\\1+', r'\\1\\1', review_text)\n",
        "  return review\n",
        "\n",
        "def repetition(review_text):\n",
        "  repeat = re.sub(r'\\b(\\w+)(?:\\W\\1\\b)+', r'\\1',review_text, flags=re.IGNORECASE)\n",
        "  return repeat\n",
        "\n",
        "def remove_extra_whitespaces(review_text):\n",
        "  review = re.sub(r'\\s+',' ', review_text)\n",
        "  return review\n",
        "\n",
        "def remove_mention(review_text) :\n",
        "  return p.clean(review_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEuxkZYO5icq"
      },
      "outputs": [],
      "source": [
        "def postgrab(url, params) :\n",
        "  # Send an HTTP POST request to the website\n",
        "  response = requests.post(url, data = params)\n",
        "  response.cookies = response.headers['Set-Cookie'].split(';')[0]\n",
        "\n",
        "  return response\n",
        "\n",
        "def getgrab(url, cookie) :\n",
        "  # Send an HTTP GET request to the website\n",
        "  response = ''\n",
        "  headers = {'Content-Type' : 'text/html; charset=UTF-8',\n",
        "             'Accept-Encoding' : 'gzip, deflate, br'}\n",
        "\n",
        "  try:\n",
        "    response = requests.get(url, cookies=cookie, headers=headers)\n",
        "    #print(response.encoding)\n",
        "  except UnicodeDecodeError:\n",
        "    print('error : ' + url)\n",
        "\n",
        "  return response\n",
        "\n",
        "def crawler(keyword) :\n",
        "  baseurl = \"https://ulas.surakarta.go.id\"\n",
        "  mainurl = baseurl + \"/index.php?mod=widget&sub=semuaAspirasi&act=view&typ=html\"\n",
        "\n",
        "  # Srapping web ulas surakarta\n",
        "  params = {'content': keyword, 'collapse_status' : '0', 'opd' : '',\n",
        "            'pengadu' : '', 'status' : '', 'display' : '200', 'ascomponent' : '1'}\n",
        "  first_keyword = postgrab(mainurl, params)\n",
        "\n",
        "  # Parse the HTML code using BeautifulSoup\n",
        "  first_keyword_soup = BeautifulSoup(first_keyword.text, 'html.parser')\n",
        "\n",
        "  session = requests.Session()\n",
        "  cookie = first_keyword.cookies.split('=')\n",
        "  cookies = dict(GTFW35SessID=cookie[1])\n",
        "\n",
        "  # Srapping web ulas surakarta\n",
        "  mainurl = baseurl + \"/index.php?mod=widget&sub=semuaAspirasi&act=view&typ=html&mmId=114&ascomponent=1&display=200&content=\" + keyword\n",
        "  first_keyword = getgrab(mainurl, cookies)\n",
        "\n",
        "  # Parse the HTML code using BeautifulSoup\n",
        "  first_keyword_soup = BeautifulSoup(first_keyword.text, 'html.parser')\n",
        "\n",
        "  # Extract the relevant information from the HTML code\n",
        "  i = 0\n",
        "  aspirations = []\n",
        "  for a in first_keyword_soup.find_all('a', {'class':'color-asp-black'}, href=True):\n",
        "    link = baseurl + a['href']\n",
        "\n",
        "    content_first_keyword = getgrab(link, cookies)\n",
        "\n",
        "    if not isinstance(content_first_keyword, str) :\n",
        "      # Parse the HTML code using BeautifulSoup\n",
        "      content_first_keyword_soup = BeautifulSoup(content_first_keyword.text, 'html.parser')\n",
        "      for div in content_first_keyword_soup.find_all('div', {'class':'col-sm-11 col-md-11 col-xs-11',\n",
        "                                                             'style' : \"text-align:justify; font-size:14px;padding-right:15px\"}, href=False):\n",
        "        aspirations.append([div.get_text(), link])\n",
        "\n",
        "  return aspirations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2IZXgP_Bx6p"
      },
      "outputs": [],
      "source": [
        "aspirations1 = crawler('aplikasi solo destination')\n",
        "aspirations2 = crawler('aplikasi solo')\n",
        "aspirations3 = crawler('solo destination')\n",
        "\n",
        "aspirations = aspirations1 + aspirations2 + aspirations3\n",
        "\n",
        "# Store the information in a pandas dataframe\n",
        "df_ulas = pd.DataFrame(aspirations, columns=['full_text', 'link'])\n",
        "df_ulas_joined = df_ulas[['full_text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI0Oq-NXKzfB"
      },
      "outputs": [],
      "source": [
        "df_ulas_joined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anmt4dmz5qM_"
      },
      "outputs": [],
      "source": [
        "df_ulas_joined.to_csv('data_ulas.csv')\n",
        "!cp data_ulas.csv \"/content/drive/MyDrive/Bismillah TA\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjaqMzksCBZu"
      },
      "source": [
        "**Menggabungkan Data dari Semua Sumber**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uk5DXVEwCHWk"
      },
      "outputs": [],
      "source": [
        "frames = [df_twitter_all, df_gplay_joined, df_ulas_joined]\n",
        "result_reset = pd.concat(frames)\n",
        "result_reset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmnqdujK-AoV"
      },
      "outputs": [],
      "source": [
        "result_reset.to_csv('data_joined.csv')\n",
        "!cp data_joined.csv \"/content/drive/MyDrive/Bismillah TA\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuC-F8HzCSdb"
      },
      "source": [
        "**Preporcessing Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ-gy7bQCYWP"
      },
      "source": [
        "Cleaning and Casefolding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWEm-NmTCaty"
      },
      "outputs": [],
      "source": [
        "def cleaning(text) :\n",
        "  lower = to_lower(text)\n",
        "  clean = remove_emoji(lower)\n",
        "  clean = remove_hashtag(clean)\n",
        "  clean = remove_mention(clean)\n",
        "  clean = remove_punctuation(clean)\n",
        "  clean = remove_superscript(clean)\n",
        "  clean = word_repetition(clean)\n",
        "  clean = repetition(clean)\n",
        "  clean = remove_extra_whitespaces(clean)\n",
        "  return clean\n",
        "\n",
        "result_reset = result_reset.reset_index(drop=True)\n",
        "result_reset['full_text'] = result_reset['full_text'].astype('string')\n",
        "result_reset['clean'] = result_reset['full_text'].apply(lambda word : cleaning(str(word)))\n",
        "result_clean = result_reset.drop_duplicates(subset=['clean'], keep='first')\n",
        "result_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJAB09eUPzIR"
      },
      "outputs": [],
      "source": [
        "result_clean.to_csv('cleansing.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABK_dZYtChuk"
      },
      "source": [
        "Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRhBdAF9Cyi_"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "slangs = open('/content/drive/MyDrive/Bismillah TA (1)/slang rara (1).txt',\"r\",\n",
        "              encoding=\"utf-8\", errors='replace') #Membuka list kata slang dari Google Drive\n",
        "\n",
        "clear_slangs= []\n",
        "for newlines in slangs:\n",
        "  strip_re = newlines.strip(\"\\n\")\n",
        "  split = re.split(r'[:]',strip_re)\n",
        "  clear_slangs.append(split)\n",
        "\n",
        "slangs = [[k.strip(), v.strip()] for k,v in clear_slangs]\n",
        "dict_slangs = {key:values for key,values in slangs}\n",
        "dict_slangs\n",
        "\n",
        "clean_text = []\n",
        "for review in result_clean['clean']:\n",
        "  wordlist = TextBlob(review).words\n",
        "  for k,v in enumerate(wordlist):\n",
        "    if v in dict_slangs.keys():\n",
        "      wordlist[k] = dict_slangs[v]\n",
        "  clean_text.append(' '.join(wordlist))\n",
        "\n",
        "result_clean['clean'] = clean_text\n",
        "\n",
        "def remove_small_words(text):\n",
        "  text = re.sub(r'\\b\\w{1,3}\\b','',text)\n",
        "  return text\n",
        "\n",
        "result_clean['normalization'] = result_clean['clean'].apply(lambda remove: remove_small_words(str(remove)))\n",
        "\n",
        "result_clean.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68A-aoewDTyA"
      },
      "source": [
        "Stopword Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AGRQ5tlDWYb"
      },
      "outputs": [],
      "source": [
        "stopwords = StopWordRemoverFactory().get_stop_words()\n",
        "#Mengambil list stopword yang diimpor dari Sastrawi.StopWordRemover.StopWordRemoverFactor\n",
        "\n",
        "def remove_stopwords(review_text) :\n",
        "  newphrase = [w for w in review_text.split(\" \") if not w in stopwords]\n",
        "  return \" \".join(newphrase)\n",
        "\n",
        "result_clean['stopwords'] = result_clean['normalization'].apply(lambda stopwords:remove_stopwords(str(stopwords)))\n",
        "\n",
        "result_clean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkaKsnipDurG"
      },
      "source": [
        "Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNQ_JBFxDwV4"
      },
      "outputs": [],
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "stemmer = StemmerFactory().create_stemmer() #stemmer dari Sastrawi.Stemmer.StemmerFactory\n",
        "def stemming(review_text) :\n",
        "  return stemmer.stem(review_text)\n",
        "\n",
        "result_clean['stemming'] = result_clean['stopwords'].apply(lambda stem:stemming(str(stem)))\n",
        "\n",
        "result_clean.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sM-44X64DrTw"
      },
      "outputs": [],
      "source": [
        "result_clean.to_csv('Preprocessing.csv')\n",
        "!cp Preprocessing.csv \"/content/drive/MyDrive/Bismillah TA\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKz9aYOUEFO_"
      },
      "source": [
        "Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wpjdG50EGiG"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def word_token(review_text): #word_tokenize dari NLTK\n",
        "  return word_tokenize(review_text)\n",
        "\n",
        "result_clean['tokenizing'] = result_clean['stemming'].apply(lambda tokenize:word_token(str(tokenize)))\n",
        "\n",
        "result_clean.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvlnAERz8udG"
      },
      "outputs": [],
      "source": [
        "#df = pd.DataFrame(result_clean.stemming) #translate data ke dalam bahasa Inggris\n",
        "#translator = Translator()\n",
        "#translations = {}\n",
        "#for column in df.columns:\n",
        "  #unique_elements = df[column].unique()\n",
        "  #for element in unique_elements:\n",
        "    #translations[element] = translator.translate(element).text\n",
        "\n",
        "#result_clean['translated_text'] = df.replace(translations)\n",
        "\n",
        "#result_clean.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncY0hU5A6yHL"
      },
      "outputs": [],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9zehe6Q82wG"
      },
      "outputs": [],
      "source": [
        "# result_clean.to_csv('Labelling Data Asli.csv')\n",
        "# !cp Preprocessing.csv \"/content/drive/MyDrive/Bismillah TA\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pj3hF-hmaAbf"
      },
      "outputs": [],
      "source": [
        "result_clean_joined = result_clean[['stemming']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro-M1norFuge"
      },
      "source": [
        "**Augmented Text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YayC-quF1Xmb"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import random\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word, lang='ind', ):\n",
        "        for lemma in syn.lemmas('ind'):\n",
        "            synonyms.add(lemma.name())\n",
        "    if word in synonyms:\n",
        "        synonyms.remove(word)\n",
        "    return list(synonyms)\n",
        "\n",
        "def synonym_replacement(sentence, n=1):\n",
        "    words = sentence.split()\n",
        "    new_words = words.copy()\n",
        "    random_word_list = list(set([word for word in words if wordnet.synsets(word, lang='ind')]))\n",
        "    random.shuffle(random_word_list)\n",
        "    num_replaced = 0\n",
        "\n",
        "    for random_word in random_word_list:\n",
        "        synonyms = get_synonyms(random_word)\n",
        "        if len(synonyms) >= 1:\n",
        "            synonym = random.choice(synonyms)\n",
        "            new_words = [synonym if word == random_word else word for word in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:  # Only replace up to n words\n",
        "            break\n",
        "\n",
        "    sentence = ' '.join(new_words)\n",
        "    return sentence\n",
        "\n",
        "def augmentating(sentence, naug=16) :\n",
        "  list_augmented_sentences = []\n",
        "\n",
        "  for i in range(naug) :\n",
        "    list_augmented_sentences.append(synonym_replacement\n",
        "     (sentence, n= 0.05 * len(sentence.split())))\n",
        "\n",
        "  return list_augmented_sentences\n",
        "\n",
        "# Looping dataframe for augmetation\n",
        "result_generate_augmented = []\n",
        "for index, row in result_clean.iterrows() :\n",
        "  augmented_sentences = augmentating(row['stemming'], 16)\n",
        "\n",
        "  for i in augmented_sentences :\n",
        "    result_generate_augmented.append(i)\n",
        "\n",
        "result_augmented = pd.DataFrame(result_generate_augmented, columns=['stemming'])\n",
        "result_augmented.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7ES4ZPSaULt"
      },
      "outputs": [],
      "source": [
        "result_augmented_joined = result_augmented[['stemming']]\n",
        "result_augmented_joined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAL2fRHraBwL"
      },
      "outputs": [],
      "source": [
        "#result_clean_translated = pd.read_csv(\"/content/drive/MyDrive/Bismillah TA/GPTTranslation.csv\")\n",
        "#gpt_translation = result_clean_translated[['translated']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikzDe_0xawmg"
      },
      "outputs": [],
      "source": [
        "frames = [result_clean_joined, result_augmented_joined]\n",
        "result = pd.concat(frames)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKhNn-aUOBqj"
      },
      "outputs": [],
      "source": [
        "result.to_csv('Augmented.csv')\n",
        "!cp Augmented.csv \"/content/drive/MyDrive/Bismillah TA\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iqQcCsGTMns"
      },
      "source": [
        "Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHx6lnE4r8N2"
      },
      "outputs": [],
      "source": [
        "# Chunk result\n",
        "n = 1000  #chunk row size\n",
        "result_chunk = [result[i:i+n] for i in range(0, result.shape[0],n)]\n",
        "\n",
        "print(result_chunk[0].shape)\n",
        "print(result_chunk[1].shape)\n",
        "print(result_chunk[2].shape)\n",
        "print(result_chunk[3].shape)\n",
        "print(result_chunk[4].shape)\n",
        "print(result_chunk[5].shape)\n",
        "print(result_chunk[6].shape)\n",
        "print(result_chunk[7].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnDt0ZSsnQ4A"
      },
      "outputs": [],
      "source": [
        "n = 500  #chunk row size\n",
        "result_chunk_5 = [result_chunk[5][i:i+n] for i in range(0, result_chunk[5].shape[0],n)]\n",
        "\n",
        "print(result_chunk_5[0].shape)\n",
        "print(result_chunk_5[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eS6rhZ7nXj5"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ1HBYv4EQpg"
      },
      "source": [
        "**Labelling Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9fzFtHhIEjRN"
      },
      "outputs": [],
      "source": [
        "#df = pd.DataFrame(result.stemming) #translate data ke dalam bahasa Inggris\n",
        "#translator = Translator()\n",
        "#translations = {}\n",
        "#for column in df.columns:\n",
        "  #unique_elements = df[column].unique()\n",
        "  #for element in unique_elements:\n",
        "    #translations[element] = translator.translate(element).text\n",
        "\n",
        "#result['translated_text'] = df.replace(translations)\n",
        "\n",
        "#result.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljEQZka62xJH"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "def GPTTranslation(text) :\n",
        "  openai.api_key = \"sk-proj-mJH34jT1v1bFDBYIFpCiT3BlbkFJpBMWViGYJQtPZ0Zkjp1w\"\n",
        "\n",
        "  # Pemanggilan API untuk terjemahan\n",
        "  chat_completion = openai.chat.completions.create(\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"Terjemahkan kata ini : {text}\",\n",
        "      }],\n",
        "    model=\"gpt-3.5-turbo-1106\",\n",
        "  )\n",
        "\n",
        "  translation = chat_completion.choices[0].message.content\n",
        "  return translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQBV9oi0341C"
      },
      "outputs": [],
      "source": [
        "# Blok code ini sengaja dikomen agar tidak request ke GPT\n",
        "#result['translated'] = result['stemming'].apply(lambda word : GPTTranslation(str(word)))\n",
        "#result_chunk[0]['translated'] = result_chunk[0]['stemming'].apply(lambda word : GPTTranslation(str(word)))\n",
        "#result_chunk[1]['translated'] = result_chunk[1]['stemming'].apply(lambda word : GPTTranslation(str(word)))\n",
        "#result_chunk[2]['translated'] = result_chunk[2]['stemming'].apply(lambda word : GPTTranslation(str(word)))\n",
        "#result_chunk[3]['translated'] = result_chunk[3]['stemming'].apply(lambda word : GPTTranslation(str(word)))\n",
        "#result_chunk[4]['translated'] = result_chunk[4]['stemming'].apply(lambda word : GPTTranslation(str(word)))\n",
        "#result_chunk[5]['translated'] = result_chunk[5]['stemming'].apply(lambda word : GPTTranslation(str(word)))\n",
        "#result_chunk[6]['translated'] = result_chunk[6]['stemming'].apply(lambda word : GPTTranslation(str(word)))\n",
        "#result_chunk[7]['translated'] = result_chunk[7]['stemming'].apply(lambda word : GPTTranslation(str(word)))\n",
        "\n",
        "#result_chunk_5[0]['translated'] = result_chunk_5[0]['stemming'].apply(lambda word : GPTTranslation(str(word)))\n",
        "#result_chunk_5[1]['translated'] = result_chunk_5[1]['stemming'].apply(lambda word : GPTTranslation(str(word)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3deSyJ0AgT_2"
      },
      "outputs": [],
      "source": [
        "#result_chunk[0].to_csv('GPTTranslation1.csv')\n",
        "#result_chunk[1].to_csv('GPTTranslation2.csv')\n",
        "#result_chunk[2].to_csv('GPTTranslation3.csv')\n",
        "#result_chunk[3].to_csv('GPTTranslation4.csv')\n",
        "#result_chunk[4].to_csv('GPTTranslation5.csv')\n",
        "#result_chunk[5].to_csv('GPTTranslation6.csv')\n",
        "#result_chunk[6].to_csv('GPTTranslation7.csv')\n",
        "#result_chunk[7].to_csv('GPTTranslation8.csv')\n",
        "\n",
        "#result_chunk_5[0].to_csv('GPTTranslation5_1.csv')\n",
        "#result_chunk_5[1].to_csv('GPTTranslation5_2.csv')\n",
        "\n",
        "#!cp GPTTranslation1.csv \"/content/drive/MyDrive/Bismillah TA\"\n",
        "#!cp GPTTranslation2.csv \"/content/drive/MyDrive/Bismillah TA\"\n",
        "#!cp GPTTranslation3.csv \"/content/drive/MyDrive/Bismillah TA\"\n",
        "#!cp GPTTranslation4.csv \"/content/drive/MyDrive/Bismillah TA\"\n",
        "#!cp GPTTranslation5.csv \"/content/drive/MyDrive/Bismillah TA\"\n",
        "#!cp GPTTranslation6.csv \"/content/drive/MyDrive/Bismillah TA\"\n",
        "#!cp GPTTranslation7.csv \"/content/drive/MyDrive/Bismillah TA\"\n",
        "#!cp GPTTranslation8.csv \"/content/drive/MyDrive/Bismillah TA\"\n",
        "\n",
        "#!cp GPTTranslation5_1.csv \"/content/drive/MyDrive/Bismillah TA\"\n",
        "#!cp GPTTranslation5_2.csv \"/content/drive/MyDrive/Bismillah TA\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1QG67BrMgPi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_l9UgKuuwUR"
      },
      "outputs": [],
      "source": [
        "result_1 = pd.read_csv(\"/content/drive/MyDrive/Bismillah TA/GPTTranslation1.csv\")\n",
        "result_2 = pd.read_csv(\"/content/drive/MyDrive/Bismillah TA/GPTTranslation2.csv\")\n",
        "result_3 = pd.read_csv(\"/content/drive/MyDrive/Bismillah TA/GPTTranslation3.csv\")\n",
        "result_4 = pd.read_csv(\"/content/drive/MyDrive/Bismillah TA/GPTTranslation4.csv\")\n",
        "result_5_1 = pd.read_csv(\"/content/drive/MyDrive/Bismillah TA/GPTTranslation5_1.csv\")\n",
        "result_5_2 = pd.read_csv(\"/content/drive/MyDrive/Bismillah TA/GPTTranslation5_2.csv\")\n",
        "result_6 = pd.read_csv(\"/content/drive/MyDrive/Bismillah TA/GPTTranslation6.csv\")\n",
        "result_7= pd.read_csv(\"/content/drive/MyDrive/Bismillah TA/GPTTranslation7.csv\")\n",
        "result_8= pd.read_csv(\"/content/drive/MyDrive/Bismillah TA/GPTTranslation8.csv\")\n",
        "frames = [result_1, result_2, result_3, result_4, result_5_1, result_5_2, result_6, result_7, result_8]\n",
        "result_gpt_translation = pd.concat(frames)\n",
        "result_gpt_translation.head()\n",
        "#gpt_translation = result_clean_translated[['translated']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2Tu57r0WVk7"
      },
      "outputs": [],
      "source": [
        "result_translated = result_gpt_translation.drop(\"tokenizing\", axis=1)\n",
        "result_translated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1fmF2wAWmUd"
      },
      "outputs": [],
      "source": [
        "result_translated.drop(\"Unnamed: 0\", axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-3j2CC5XZwV"
      },
      "outputs": [],
      "source": [
        "result_translated.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvATfOvZZNZX"
      },
      "outputs": [],
      "source": [
        "result_translated['translated'] = result_translated['translated'].fillna('').astype(str)\n",
        "result_translated['stemming'] = result_translated['stemming'].fillna('').astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcuVjsUuE-lp"
      },
      "outputs": [],
      "source": [
        " def subjektivitas(tr_text):\n",
        "   return TextBlob(tr_text).sentiment.subjectivity\n",
        "\n",
        " def polaritas(tr_text):\n",
        "   return TextBlob(tr_text).sentiment.polarity\n",
        "\n",
        " def hasilSentimen(nilai):\n",
        "   if nilai < 0:\n",
        "     return 0\n",
        "   else:\n",
        "     return 1\n",
        "\n",
        " result_translated['subjektivitas'] = result_translated['translated'].apply(subjektivitas)\n",
        " result_translated['polaritas'] = result_translated['translated'].apply(polaritas)\n",
        " result_translated['sentimen'] = result_translated['polaritas'].apply(hasilSentimen)\n",
        "\n",
        " result_translated.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CizL05M-XX9W"
      },
      "outputs": [],
      "source": [
        " result_translated.to_csv('Labelling.csv')\n",
        "!cp Labelling.csv \"/content/drive/MyDrive/Bismillah TA\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYS72frdFPKi"
      },
      "outputs": [],
      "source": [
        " result_translated['sentimen'].value_counts() #Menghitung jumlah sentimen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CCEoyVXTSjF"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def word_token(review_text): #word_tokenize dari NLTK\n",
        "  return word_tokenize(review_text)\n",
        "\n",
        "result_translated['tokenizing'] =  result_translated['stemming'].apply(lambda tokenize:word_token(str(tokenize)))\n",
        "result_translated.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGbnkzTkFsT6"
      },
      "source": [
        "**Split Data menjadi Data Training dan Data Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpKeuFheFxIZ"
      },
      "outputs": [],
      "source": [
        "data_train, data_test = train_test_split(result_translated, test_size=0.20, random_state=42)\n",
        "#membagi data menjadi data training sebesar 80% dan data testing sebesar 20%\n",
        "print(\"Data train : %s, Data test:  %s.\" % (data_train.shape, data_test.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5PU9P5WNDV0"
      },
      "source": [
        "**Check Vocab and Max Sentence Lenght**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKZKcBZxGEIy"
      },
      "outputs": [],
      "source": [
        "data_train_words = [word for tokenizing in data_train[\"tokenizing\"] for word in tokenizing]\n",
        "training_sentence_lenghts = [len(tokenizing) for tokenizing in data_train[\"tokenizing\"]]\n",
        "TRAINING_VOCAB = sorted(list(set(data_train_words)))\n",
        "print(\"%s total kata, dengan total keseluruhan kata sebanyak %s\" % (len(data_train_words), len(TRAINING_VOCAB)))\n",
        "print(\"panjang maksimal kata adalah %s\" % max(training_sentence_lenghts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmkkV1bPHbOE"
      },
      "outputs": [],
      "source": [
        "data_test_words = [word for tokenizing in data_test[\"tokenizing\"] for word in tokenizing]\n",
        "test_sentence_lenghts = [len(tokenizing) for tokenizing in data_test[\"tokenizing\"]]\n",
        "TEST_VOCAB = sorted(list(set(data_test_words)))\n",
        "print(\"%s total kata, dengan total keseluruhan kata sebanyak %s\" % (len(data_test_words), len(TEST_VOCAB)))\n",
        "print(\"panjang maksimal kata adalah %s\" % max(test_sentence_lenghts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b96E74ExH1GR"
      },
      "source": [
        "**Word Embedding Using Word2Vec model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uMfMumZH-Ki"
      },
      "outputs": [],
      "source": [
        "word2vec_path = '/content/drive/MyDrive/Bismillah TA/GoogleNews-vectors-negative300.bin.gz'\n",
        "#File biner yang berisi kumpulan vektor kata\n",
        "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKBnplj2IFiP"
      },
      "outputs": [],
      "source": [
        "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
        "    if len(tokens_list)<1:\n",
        "        return np.zeros(k)\n",
        "    if generate_missing:\n",
        "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
        "    else:\n",
        "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
        "    length = len(vectorized)\n",
        "    summed = np.sum(vectorized, axis=0)\n",
        "    averaged = np.divide(summed, length)\n",
        "    return averaged\n",
        "\n",
        "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
        "    embeddings = clean_comments['tokenizing'].apply(lambda x: get_average_word2vec(x, vectors,\n",
        "                                                                                generate_missing=generate_missing))\n",
        "    return list(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxYoXFn1IKzL"
      },
      "source": [
        "**Get Embedding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSFC9KOtIOd1"
      },
      "outputs": [],
      "source": [
        "training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKrIEZNoOtJV"
      },
      "outputs": [],
      "source": [
        "data_train['doc_len'] = data_train['stemming'].apply(lambda words: len(words.split(' ')))\n",
        "max_seq_len = np.max(data_train['doc_len'])+1 #Mengetahui jumlah maksimal panjang dari sequence kata\n",
        "max_seq_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nARbXkU3IV8f"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7r7dSmFPM26"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "\n",
        "data_train['doc_len'].plot(kind='hist',\n",
        "                      density=True,\n",
        "                      alpha=0.65,\n",
        "                      bins=15,\n",
        "                      label=\"Content's Frequency\")\n",
        "\n",
        "data_train['doc_len'].plot(kind='kde', label='')\n",
        "\n",
        "ax.set_xlim(-5, 22)\n",
        "ax.set_xlabel(\"\")\n",
        "ax.set_ylim(0, 0.13)\n",
        "ax.set_yticks([])\n",
        "ax.set_ylabel(\"\")\n",
        "ax.set_title(\"Word Distribution per Content\")\n",
        "ax.grid(False)\n",
        "ax.axvline(x=max_seq_len, alpha=0.65, color='k', linestyle=':', label='Max Sequence length')\n",
        "ax.tick_params(left = False, bottom = False)\n",
        "for ax, spine in ax.spines.items():\n",
        "    spine.set_visible(False)\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWBcThEcIlSc"
      },
      "source": [
        "**Tokenize and Pad Sequences**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jxg037gcIn8W"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
        "#Mengonversi kata-kata menjadi bilangan numerik dengan jumlah kata yang sering muncul sebanyak num_words\n",
        "tokenizer.fit_on_texts(data_train[\"stemming\"].tolist())\n",
        "training_sequences = tokenizer.texts_to_sequences(data_train[\"stemming\"].tolist())\n",
        "#mengubah kalimat pada data train kolom 'stemming' ke dalam nilai yang sesuai dengan fungsi texts_to_sequences.\n",
        "\n",
        "train_word_index = tokenizer.word_index #Melihat hasil tokenisasi\n",
        "print('Found %s unique tokens.' % len(train_word_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_f2uSbSPIs0z"
      },
      "outputs": [],
      "source": [
        "train_cnn_data = pad_sequences(training_sequences, maxlen=max_seq_len)\n",
        "#Mengubah setiap kalimat pada teks dengan panjang yang sama, dengan panjang maksimal adalah max_seq_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwALjhA2I45l"
      },
      "outputs": [],
      "source": [
        "test_sequences = tokenizer.texts_to_sequences(data_test[\"stemming\"].tolist())\n",
        "test_cnn_data = pad_sequences(test_sequences, maxlen=max_seq_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPzFFbaRUfWs"
      },
      "source": [
        "**Train Embeddings Weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCdBYMA6Ukui"
      },
      "outputs": [],
      "source": [
        "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM)) #\n",
        "for word,index in train_word_index.items():\n",
        "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
        "    #menemukan vektor bobot embedding pada word2vec embedding yang telah disematkan\n",
        "print(train_embedding_weights.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYChiO396YSL"
      },
      "source": [
        "**Word Contextualized Embedding Using IndoBERT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yzg8xP2M7gXF"
      },
      "outputs": [],
      "source": [
        "# Memnggil model dan tokenizer IndoBERT\n",
        "#bert_model = TFAutoModel.from_pretrained(\"indobenchmark/indobert-base-p2\", trainable=False)\n",
        "#bert_tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2qGD76KAD3I"
      },
      "outputs": [],
      "source": [
        "# Pendefinisian fungsi untuk melakukan tokenisasi pada satu data\n",
        "#def tokenisasi(teks):\n",
        "      #encode_dict = bert_tokenizer(teks,\n",
        "                                   #add_special_tokens = True,\n",
        "                                   #max_length = 128, #maximum token per kalimat = 125\n",
        "                                   #padding = 'max_length',\n",
        "                                   #truncation = True,\n",
        "                                   #return_attention_mask = True,\n",
        "                                   #return_tensors = 'tf',)\n",
        "\n",
        "      #tokenID = encode_dict['input_ids']\n",
        "      #attention_mask = encode_dict['attention_mask']\n",
        "\n",
        "      #return tokenID, attention_mask\n",
        "\n",
        "\n",
        "# Pendefinisian fungsi untuk mengambil hasil tokenisasi pada semua data\n",
        "#def create_input(data):\n",
        "    #tokenID, input_mask = [], []\n",
        "    #for teks in data:\n",
        "        #token, mask = tokenisasi(teks)\n",
        "        #tokenID.append(token)\n",
        "        #input_mask.append(mask)\n",
        "\n",
        "    #return [np.asarray(tokenID, dtype=np.int32).reshape(-1, 128),\n",
        "            #np.asarray(input_mask, dtype=np.int32).reshape(-1, 128)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNMhiA99DViG"
      },
      "outputs": [],
      "source": [
        "#bert_label = result_clean.drop(['sentimen'], axis=1)\n",
        "#data_train_bert, data_test_bert, label_train_bert, label_test_bert = train_test_split(result_clean, bert_label, test_size=0.2, random_state=42)\n",
        "\n",
        "#x_train_bert = create_input(data_train_bert)\n",
        "#x_test_bert = create_input(data_test_bert)\n",
        "\n",
        "#print(result_clean.shape)\n",
        "#print(bert_label.shape)\n",
        "\n",
        "#print(x_train_bert[0].shape, x_train_bert[1].shape, label_train_bert.shape)\n",
        "#print(x_test_bert[0].shape, x_test_bert[1].shape, label_test_bert.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiIRnd9YMeYS"
      },
      "outputs": [],
      "source": [
        "#Input layer\n",
        "#input_token = keras.layers.Input(shape=(128,), dtype=np.int32,name=\"input_token\")\n",
        "#input_mask = keras.layers.Input(shape=(128,), dtype=np.int32,name=\"input_mask\")\n",
        "\n",
        "#print(input_token)\n",
        "# Embedding\n",
        "#bert_embedding = bert_model([input_token, input_mask])[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orcq8Xc6I6c5"
      },
      "source": [
        "**Define CNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnk23mdCQWnh"
      },
      "source": [
        "Hyperparamater CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RU7K5gFQZJu"
      },
      "outputs": [],
      "source": [
        "learning_rate= 0.001\n",
        "batch_size  = 64\n",
        "num_epochs  = 50\n",
        "num_filters = 256\n",
        "filter_size = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9WcMMmG64ya"
      },
      "outputs": [],
      "source": [
        "nb_words = len(train_word_index)\n",
        "nb_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2q5VTj3HQ2kJ"
      },
      "outputs": [],
      "source": [
        "label_names = ['positive','negative']\n",
        "num_classes = len(label_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzG5lhM6Q7tw"
      },
      "outputs": [],
      "source": [
        "target = []\n",
        "\n",
        "for i in data_train['sentimen']:\n",
        "  if i == 0:\n",
        "    target.append(0)\n",
        "  elif i == 1:\n",
        "    target.append(1)\n",
        "\n",
        "data_train['new_label'] = target\n",
        "\n",
        "train_labels = to_categorical(data_train['new_label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnhVHWqsRguH"
      },
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping(monitor='loss',\n",
        "                               min_delta=0.01,\n",
        "                               patience=3,\n",
        "                               verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
        "                              patience=6,\n",
        "                              factor=0.1,\n",
        "                              verbose=2)\n",
        "mcp = ModelCheckpoint(filepath = 'cnn_model_wights.h5',\n",
        "                      monitor = 'val_categorical_accuracy',\n",
        "                      save_best_only = True,\n",
        "                      verbose = 1)\n",
        "\n",
        "callbacks_list = [early_stopping, reduce_lr, mcp]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWBa0Ei4TAGn"
      },
      "source": [
        "Model CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIxw1tbbR5r9"
      },
      "outputs": [],
      "source": [
        "model = Sequential(name='model')\n",
        "\n",
        "model.add(Embedding(nb_words+1, EMBEDDING_DIM,\n",
        "                    weights=[train_embedding_weights],\n",
        "                    input_length=max_seq_len,\n",
        "                    trainable=False))\n",
        "\n",
        "model.add(Conv1D(num_filters, filter_size, activation='relu', padding='same', strides=1))\n",
        "model.add(AveragePooling1D(2))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "\n",
        "model.add(Conv1D(num_filters, filter_size, activation='relu', padding='same', strides=1))\n",
        "model.add(AveragePooling1D(2))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(GlobalAveragePooling1D())\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-4)))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "adam = tf.optimizers.Adam(learning_rate)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cimo6RkJJMsf"
      },
      "source": [
        "**Train CNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYo8o-dEJTPb"
      },
      "outputs": [],
      "source": [
        "hist = model.fit(train_cnn_data, train_labels, batch_size = batch_size, epochs = num_epochs, validation_split=0.2, callbacks = callbacks_list, shuffle=True, verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpYibP14JWmH"
      },
      "source": [
        "**Test CNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OodeQc9kJYuw"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(test_cnn_data, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWABleNdJdSf"
      },
      "outputs": [],
      "source": [
        "labels = [1, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhLO43kNJiTS"
      },
      "outputs": [],
      "source": [
        "prediction_labels=[]\n",
        "for p in y_pred:\n",
        "    prediction_labels.append(labels[np.argmax(p)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6BB9gP1JkCB"
      },
      "outputs": [],
      "source": [
        "sum(data_test.sentimen==prediction_labels)/len(prediction_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNuI7-M_wGPi"
      },
      "outputs": [],
      "source": [
        "target_test = []\n",
        "\n",
        "for i in data_test['sentimen']:\n",
        "  if i == 0:\n",
        "    target_test.append(0)\n",
        "  elif i == 1:\n",
        "    target_test.append(1)\n",
        "\n",
        "data_test['new_label'] = target_test\n",
        "\n",
        "test_labels = to_categorical(data_test['new_label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbMZUFNjx9DR"
      },
      "outputs": [],
      "source": [
        "y_test = data_test['sentimen']\n",
        "\n",
        "y_pred = model.predict(test_cnn_data).round()\n",
        "pred_df = pd.DataFrame(data=y_pred)\n",
        "y_pred = pred_df.values.argmax(1)\n",
        "pred_df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEZY2z9a6_b1"
      },
      "source": [
        "**EVALUATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qLe-SLZqwLy"
      },
      "outputs": [],
      "source": [
        "train_loss, train_accuracy = model.evaluate(train_cnn_data,train_labels)\n",
        "test_loss, test_accuracy = model.evaluate(test_cnn_data, test_labels)\n",
        "\n",
        "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Training Loss: {train_loss:.4f}\")\n",
        "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Testing Loss: {test_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ai4h6DzevrJi"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_pred, digits= 6))\n",
        "\n",
        "plot_confusion_matrix(confusion_matrix(y_test, y_pred))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnyHSjBN-ajx"
      },
      "source": [
        "**Upload Predicted Files to Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oX0wVyXl_rKk"
      },
      "outputs": [],
      "source": [
        "predict = pd.DataFrame(columns=['text'] + ['label'] + ['predict'])\n",
        "predict['text'] = data_test['stemming']\n",
        "predict['label'] = y_test\n",
        "predict['predict'] = y_pred\n",
        "predict_all_negatif = predict[(predict['label'] == 0) & (predict['predict'] == 0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSxmVJogKdt4"
      },
      "outputs": [],
      "source": [
        "predict_all_negatif.to_csv('Negatif_to_Negatif.csv')\n",
        "!cp Negatif_to_Negatif.csv \"/content/drive/MyDrive/Bismillah TA\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlHIB4uKWGdR"
      },
      "outputs": [],
      "source": [
        "predict = pd.DataFrame(columns=['text'] + ['label'] + ['predict'])\n",
        "predict['text'] = data_test['stemming']\n",
        "predict['label'] = y_test\n",
        "predict['predict'] = y_pred\n",
        "predict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict['label'].value_counts()"
      ],
      "metadata": {
        "id": "nC5V7ejs30bQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axxZ4QSzWtQ3"
      },
      "outputs": [],
      "source": [
        "predict = pd.DataFrame(columns=['text'] + ['label'] + ['predict'])\n",
        "predict['text'] = data_test['stemming']\n",
        "predict['label'] = y_test\n",
        "predict['predict'] = y_pred\n",
        "predict[(predict['label'] == 1) & (predict['predict'] == 1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmsoT-l9LGVc"
      },
      "outputs": [],
      "source": [
        "predict.to_csv('Positif_to_Positif.csv')\n",
        "!cp Positif_to_Positif.csv \"/content/drive/MyDrive/Bismillah TA\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNso4SlcbgNd"
      },
      "outputs": [],
      "source": [
        "predict = pd.DataFrame(columns=['text'] + ['label'] + ['predict'])\n",
        "predict['text'] = data_test['stemming']\n",
        "predict['label'] = y_test\n",
        "predict['predict'] = y_pred\n",
        "predict_negatif_positif = predict[(predict['label'] == 0) & (predict['predict'] == 1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0ukwOs0LMks"
      },
      "outputs": [],
      "source": [
        "predict_negatif_positif.to_csv('Negatif_to_Positif.csv')\n",
        "!cp Negatif_to_Positif.csv \"/content/drive/MyDrive/Bismillah TA\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGcmn9gkbm_6"
      },
      "outputs": [],
      "source": [
        "predict = pd.DataFrame(columns=['text'] + ['label'] + ['predict'])\n",
        "predict['text'] = data_test['stemming']\n",
        "predict['label'] = y_test\n",
        "predict['predict'] = y_pred\n",
        "positif_to_negatif = predict[(predict['label'] == 1) & (predict['predict'] == 0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_9tRvgpLR07"
      },
      "outputs": [],
      "source": [
        "positif_to_negatif.to_csv('Positif_to_Negatif.csv')\n",
        "!cp Positif_to_Negatif.csv \"/content/drive/MyDrive/Bismillah TA\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p88JRsWmAcMJ"
      },
      "outputs": [],
      "source": [
        "predict.to_excel('new_predict.xlsx')\n",
        "!cp new_predict.xlsx '/content/drive/MyDrive/Bismillah TA'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1rvGRupAnfe"
      },
      "source": [
        "**Loss and Accuracy Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6ydb9UXA0lp"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "ax1.plot(hist.history['loss'], lw=2.0, color='b', alpha=0.65, label='train')\n",
        "ax1.plot(hist.history['val_loss'], lw=2.0, color='r', alpha=0.65, label='val')\n",
        "ax1.set_title('Loss Visualization')\n",
        "ax1.set_xlabel('Epochs')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend(loc='upper right')\n",
        "ax2.plot(hist.history['categorical_accuracy'], lw=2.0, color='b', alpha=0.65, label='train')\n",
        "ax2.plot(hist.history['val_categorical_accuracy'], lw=2.0, color='r', alpha=0.65, label='val')\n",
        "ax2.set_title('Accuracy Visualization')\n",
        "ax2.set_xlabel('Epochs')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend(loc='lower right')\n",
        "\n",
        "ax1.grid(False)\n",
        "ax2.grid(False)\n",
        "\n",
        "ax2.set_ylim(-0.1, 1.1)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyKiM1H-qcwq"
      },
      "source": [
        "**Word Cloud**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqPw_cDQhMJQ"
      },
      "outputs": [],
      "source": [
        "result_positive = result[result['sentimen'] == 1]\n",
        "result_negative = result[result['sentimen'] == 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyB_-_ZVidFj"
      },
      "outputs": [],
      "source": [
        "#word cloud positive\n",
        "it = iter(result_positive['stemming'])\n",
        "text = [next(it).join(i) for i in result_positive['stemming']]\n",
        "\n",
        "# lower max_font_size, change the maximum number of word and lighten the background:\n",
        "wordcloud = WordCloud(max_font_size=50, max_words=20, background_color=\"white\").generate(str(text))\n",
        "plt.figure()\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcytU4yZqkLt"
      },
      "outputs": [],
      "source": [
        "#word cloud negative\n",
        "it = iter(result_negative['stemming'])\n",
        "text = [next(it).join(i) for i in result_negative['stemming']]\n",
        "\n",
        "# lower max_font_size, change the maximum number of word and lighten the background:\n",
        "wordcloud = WordCloud(max_font_size=50, max_words=20, background_color=\"white\").generate(str(text))\n",
        "plt.figure()\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
